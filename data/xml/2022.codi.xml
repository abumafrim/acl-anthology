<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.codi">
  <volume id="1" ingest-date="2022-10-06">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Computational Approaches to Discourse</booktitle>
      <editor><first>Chloe</first><last>Braud</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Junyi Jessy</first><last>Li</last></editor>
      <editor><first>Sharid</first><last>Loaiciga</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Amir</first><last>Zeldes</last></editor>
      <publisher>International Conference on Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea and Online</address>
      <month>October</month>
      <year>2022</year>
      <url hash="08f390cd">2022.codi-1</url>
    </meta>
    <frontmatter>
      <url hash="d0f8eb8f">2022.codi-1.0</url>
      <bibkey>codi-2022-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>KOJAK</fixed-case>: A New Corpus for Studying <fixed-case>G</fixed-case>erman Discourse Particle ja</title>
      <author><first>Adil</first><last>Soubki</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Chong</first><last>Kang</last></author>
      <pages>1–6</pages>
      <abstract>In German, ja can be used as a discourse particle to indicate that a proposition, according to the speaker, is believed by both the speaker and audience. We use this observation to create KoJaK, a distantly-labeled English dataset derived from Europarl for studying when a speaker believes a statement to be common ground. This corpus is then analyzed to identify lexical choices in English that correspond with German ja. Finally, we perform experiments on the dataset to predict if an English clause corresponds to a German clause containing ja and achieve an F-measure of 75.3% on a balanced test corpus.</abstract>
      <url hash="c999b1a1">2022.codi-1.1</url>
      <bibkey>soubki-etal-2022-kojak</bibkey>
    </paper>
    <paper id="2">
      <title>Improving Topic Segmentation by Injecting Discourse Dependencies</title>
      <author><first>Linzi</first><last>Xing</last></author>
      <author><first>Patrick</first><last>Huber</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>7–18</pages>
      <abstract>Recent neural supervised topic segmentation models achieve distinguished superior effectiveness over unsupervised methods, with the availability of large-scale training corpora sampled from Wikipedia. These models may, however, suffer from limited robustness and transferability caused by exploiting simple linguistic cues for prediction, but overlooking more important inter-sentential topical consistency. To address this issue, we present a discourse-aware neural topic segmentation model with the injection of above-sentence discourse dependency structures to encourage the model make topic boundary prediction based more on the topical consistency between sentences. Our empirical study on English evaluation datasets shows that injecting above-sentence discourse structures to a neural topic segmenter with our proposed strategy can substantially improve its performances on intra-domain and out-of-domain data, with little increase of model’s complexity.</abstract>
      <url hash="2afff751">2022.codi-1.2</url>
      <bibkey>xing-etal-2022-improving</bibkey>
    </paper>
    <paper id="3">
      <title>Evaluating How Users Game and Display Conversation with Human-Like Agents</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Soomin</first><last>Kim</last></author>
      <author><first>Eujeong</first><last>Choi</last></author>
      <author><first>Younghoon</first><last>Jeong</last></author>
      <pages>19–27</pages>
      <abstract>Recently, with the advent of high-performance generative language models, artificial agents that communicate directly with the users have become more human-like. This development allows users to perform a diverse range of trials with the agents, and the responses are sometimes displayed online by users who share or show-off their experiences. In this study, we explore dialogues with a social chatbot uploaded to an online community, with the aim of understanding how users game human-like agents and display their conversations. Having done this, we assert that user postings can be investigated from two aspects, namely conversation topic and purpose of testing, and suggest a categorization scheme for the analysis. We analyze 639 dialogues to develop an annotation protocol for the evaluation, and measure the agreement to demonstrate the validity. We find that the dialogue content does not necessarily reflect the purpose of testing, and also that users come up with creative strategies to game the agent without being penalized.</abstract>
      <url hash="242cef06">2022.codi-1.3</url>
      <bibkey>cho-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="4">
      <title>Evaluating Discourse Cohesion in Pre-trained Language Models</title>
      <author><first>Jie</first><last>He</last></author>
      <author><first>Wanqiu</first><last>Long</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>28–34</pages>
      <abstract>Large pre-trained neural models have achieved remarkable success in natural language process (NLP), inspiring a growing body of research analyzing their ability from different aspects. In this paper, we propose a test suite to evaluate the cohesive ability of pre-trained language models. The test suite contains multiple cohesion phenomena between adjacent and non-adjacent sentences. We try to compare different pre-trained language models on these phenomena and analyze the experimental results,hoping more attention can be given to discourse cohesion in the future. The built discourse cohesion test suite will be publicly available at https://github.com/probe2/discourse_cohesion.</abstract>
      <url hash="177c4fe9">2022.codi-1.4</url>
      <bibkey>he-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="5">
      <title>Easy-First Bottom-Up Discourse Parsing via Sequence Labelling</title>
      <author><first>Andrew</first><last>Shen</last></author>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>35–41</pages>
      <abstract>We propose a novel unconstrained bottom-up approach for rhetorical discourse parsing based on sequence labelling of adjacent pairs of discourse units (DUs), based on the framework of Koto et al. (2021). We describe the unique training requirements of an unconstrained parser, and explore two different training procedures: (1) fixed left-to-right; and (2) random order in tree construction. Additionally, we introduce a novel dynamic oracle for unconstrained bottom-up parsing. Our proposed parser achieves competitive results for bottom-up rhetorical discourse parsing.</abstract>
      <url hash="22540c9a">2022.codi-1.5</url>
      <bibkey>shen-etal-2022-easy</bibkey>
    </paper>
    <paper id="6">
      <title>Using Translation Process Data to Explore Explicitation and Implicitation through Discourse Connectives</title>
      <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
      <author><first>Michael</first><last>Carl</last></author>
      <pages>42–47</pages>
      <abstract>We look into English-German translation process data to analyse explicitation and implicitation phenomena of discourse connectives. For this, we use the database CRITT TPR-DB which contains translation process data with various features that elicit online translation behaviour. We explore the English-German part of the data for discourse connectives that are either omitted or inserted in the target, as well as cases when changing a weak signal to strong one, or the other way around. We determine several features that have an impact on cognitive effort during translation for explicitation and implicitation. Our results show that cognitive load caused by implicitation and explicitation may depend on the discourse connectives used, as well as on the strength and the type of the relations the connectives convey.</abstract>
      <url hash="0dd16ca1">2022.codi-1.6</url>
      <bibkey>lapshinova-koltunski-carl-2022-using</bibkey>
    </paper>
    <paper id="7">
      <title>Label distributions help implicit discourse relation classification</title>
      <author><first>Frances</first><last>Yung</last></author>
      <author><first>Kaveri</first><last>Anuranjana</last></author>
      <author><first>Merel</first><last>Scholman</last></author>
      <author><first>Vera</first><last>Demberg</last></author>
      <pages>48–53</pages>
      <abstract>Implicit discourse relations can convey more than one relation sense, but much of the research on discourse relations has focused on single relation senses. Recently, DiscoGeM, a novel multi-domain corpus, which contains 10 crowd-sourced labels per relational instance, has become available. In this paper, we analyse the co-occurrences of relations in DiscoGem and show that they are systematic and characteristic of text genre. We then test whether information on multi-label distributions in the data can help implicit relation classifiers. Our results show that incorporating multiple labels in parser training can improve its performance, and yield label distributions which are more similar to human label distributions, compared to a parser that is trained on just a single most frequent label per instance.</abstract>
      <url hash="46a103b3">2022.codi-1.7</url>
      <bibkey>yung-etal-2022-label</bibkey>
    </paper>
    <paper id="8">
      <title>The Keystone Role Played by Questions in Debate</title>
      <author><first>Zlata</first><last>Kikteva</last></author>
      <author><first>Kamila</first><last>Gorska</last></author>
      <author><first>Wassiliki</first><last>Siskou</last></author>
      <author><first>Annette</first><last>Hautli-Janisz</last></author>
      <author><first>Chris</first><last>Reed</last></author>
      <pages>54–63</pages>
      <abstract>Building on the recent results of a study into the roles that are played by questions in argumentative dialogue (Hautli-Janisz et al.,2022a), we expand the analysis to investigate a newly released corpus that constitutes the largest extant corpus of closely annotated debate. Questions play a critical role in driving dialogical discourse forward; in combative or critical discursive environments, they not only provide a range of discourse management techniques, they also scaffold the semantic structure of the positions that interlocutors develop. The boundaries, however, between providing substantive answers to questions, merely responding to questions, and evading questions entirely, are fuzzy and the way in which answers, responses and evasions affect the subsequent development of dialogue and argumentation structure are poorly understood. In this paper, we explore how questions have ramifications on the large-scale structure of a debate using as our substrate the BBC television programme Question Time, the foremost topical debate show in the UK. Analysis of the data demonstrates not only that questioning plays a particularly prominent role in such debate, but also that its repercussions can reverberate through a discourse.</abstract>
      <url hash="692654be">2022.codi-1.8</url>
      <bibkey>kikteva-etal-2022-keystone</bibkey>
    </paper>
    <paper id="9">
      <title>Shallow Discourse Parsing for Open Information Extraction and Text Simplification</title>
      <author><first>Christina</first><last>Niklaus</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <author><first>Siegfried</first><last>Handschuh</last></author>
      <pages>64–76</pages>
      <abstract>We present a discourse-aware text simplification (TS) approach that recursively splits and rephrases complex English sentences into a semantic hierarchy of simplified sentences. Using a set of linguistically principled transformation patterns, sentences are converted into a hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. As opposed to previously proposed sentence splitting approaches, which commonly do not take into account discourse-level aspects, our TS approach preserves the semantic relationship of the decomposed constituents in the output. A comparative analysis with the annotations contained in RST-DT shows that we capture the contextual hierarchy between the split sentences with a precision of 89% and reach an average precision of 69% for the classification of the rhetorical relations that hold between them. Moreover, an integration into state-of-the-art Open Information Extraction (IE) systems reveals that when applying our TS approach as a pre-processing step, the generated relational tuples are enriched with additional meta information, resulting in a novel lightweight semantic representation for the task of Open IE.</abstract>
      <url hash="14c9885c">2022.codi-1.9</url>
      <bibkey>niklaus-etal-2022-shallow</bibkey>
    </paper>
    <paper id="10">
      <title>Predicting Political Orientation in News with Latent Discourse Structure to Improve Bias Understanding</title>
      <author><first>Nicolas</first><last>Devatine</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Chloé</first><last>Braud</last></author>
      <pages>77–85</pages>
      <abstract>With the growing number of information sources, the problem of media bias becomes worrying for a democratic society. This paper explores the task of predicting the political orientation of news articles, with a goal of analyzing how bias is expressed. We demonstrate that integrating rhetorical dimensions via latent structures over sub-sentential discourse units allows for large improvements, with a +7.4 points difference between the base LSTM model and its discourse-based version, and +3 points improvement over the previous BERT-based state-of-the-art model. We also argue that this gives a new relevant handle for analyzing political bias in news articles.</abstract>
      <url hash="a24a99a8">2022.codi-1.10</url>
      <bibkey>devatine-etal-2022-predicting</bibkey>
    </paper>
    <paper id="11">
      <title>Attention Modulation for Zero-Shot Cross-Domain Dialogue State Tracking</title>
      <author><first>Mathilde</first><last>Veron</last></author>
      <author><first>Olivier</first><last>Galibert</last></author>
      <author><first>Guillaume</first><last>Bernard</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>86–91</pages>
      <abstract>Dialog state tracking (DST) is a core step for task-oriented dialogue systems aiming to track the user’s current goal during a dialogue. Recently a special focus has been put on applying existing DST models to new domains, in other words performing zero-shot cross-domain transfer. While recent state-of-the-art models leverage large pre-trained language models, no work has been made on understanding and improving the results of first developed zero-shot models like SUMBT. In this paper, we thus propose to improve SUMBT zero-shot results on MultiWOZ by using attention modulation during inference. This method improves SUMBT zero-shot results significantly on two domains and does not worsen the initial performance with the great advantage of needing no additional training.</abstract>
      <url hash="55ca6f07">2022.codi-1.11</url>
      <bibkey>veron-etal-2022-attention</bibkey>
    </paper>
    <paper id="12">
      <title>An Empirical Study of Topic Transition in Dialogue</title>
      <author><first>Mayank</first><last>Soni</last></author>
      <author><first>Brendan</first><last>Spillane</last></author>
      <author><first>Leo</first><last>Muckley</last></author>
      <author><first>Orla</first><last>Cooney</last></author>
      <author><first>Emer</first><last>Gilmartin</last></author>
      <author><first>Christian</first><last>Saam</last></author>
      <author><first>Benjamin</first><last>Cowan</last></author>
      <author><first>Vincent</first><last>Wade</last></author>
      <pages>92–99</pages>
      <abstract>Although topic transition has been studied in dialogue for decades, only a handful of corpora based quantitative studies have been conducted to investigate the nature of topic transitions. Towards this end, this study annotates 215 conversations from the switchboard corpus, perform quantitative analysis and finds that 1) longer conversations consists of more topic transitions, 2) topic transition are usually lead by one participant and 3) we found no pattern in time series progression of topic transition. We also model topic transition with a precision of 91%.</abstract>
      <url hash="8701cdd2">2022.codi-1.12</url>
      <bibkey>soni-etal-2022-empirical</bibkey>
    </paper>
  </volume>
</collection>
