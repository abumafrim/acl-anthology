<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.codicrac">
  <volume id="1" ingest-date="2022-10-06">
    <meta>
      <booktitle>Proceedings of the CODI-CRAC 2022 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</booktitle>
      <editor><first>Juntao</first><last>Yu</last></editor>
      <editor><first>Sopan</first><last>Khosla</last></editor>
      <editor><first>Ramesh</first><last>Manuvinakurike</last></editor>
      <editor><first>Lori</first><last>Levin</last></editor>
      <editor><first>Vincent</first><last>Ng</last></editor>
      <editor><first>Massimo</first><last>Poesio</last></editor>
      <editor><first>Michael</first><last>Strube</last></editor>
      <editor><first>Carolyn</first><last>Rose</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="1ddb7266">2022.codicrac-1</url>
    </meta>
    <frontmatter>
      <url hash="efe11497">2022.codicrac-1.0</url>
      <bibkey>codicrac-2022-codi</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The <fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2022 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
      <author><first>Juntao</first><last>Yu</last></author>
      <author><first>Sopan</first><last>Khosla</last></author>
      <author><first>Ramesh</first><last>Manuvinakurike</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>1–14</pages>
      <abstract>The CODI-CRAC 2022 Shared Task on Anaphora Resolution in Dialogues is the second edition of an initiative focused on detecting different types of anaphoric relations in conversations of different kinds. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on these key relations. The second edition of the shared task maintained the focus on these relations and used the same datasets as in 2021, but new test data were annotated, the 2021 data were checked, and new subtasks were added. In this paper, we discuss the annotation schemes, the datasets, the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 230 runs from three teams, with most submissions achieving significantly better results than our baseline methods.</abstract>
      <url hash="cab6dccb">2022.codicrac-1.1</url>
      <bibkey>yu-etal-2022-codi</bibkey>
    </paper>
    <paper id="2">
      <title>Anaphora Resolution in Dialogue: System Description (<fixed-case>CODI</fixed-case>-<fixed-case>CRAC</fixed-case> 2022 Shared Task)</title>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Natalia</first><last>Skachkova</last></author>
      <author><first>Joseph</first><last>Renner</last></author>
      <author><first>Priyansh</first><last>Trivedi</last></author>
      <pages>15–27</pages>
      <abstract>We describe three models submitted for the CODI-CRAC 2022 shared task. To perform identity anaphora resolution, we test several combinations of the incremental clustering approach based on the Workspace Coreference System (WCS) with other coreference models. The best result is achieved by adding the “cluster merging” version of the coref-hoi model, which brings up to 10.33% improvement1 over vanilla WCS clustering. Discourse deixis resolution is implemented as multi-task learning: we combine the learning objective of coref-hoi with anaphor type classification. We adapt the higher-order resolution model introduced in Joshi et al. (2019) for bridging resolution given gold mentions and anaphors.</abstract>
      <url hash="aa1b88f1">2022.codicrac-1.2</url>
      <bibkey>anikina-etal-2022-anaphora</bibkey>
    </paper>
    <paper id="3">
      <title>Pipeline Coreference Resolution Model for Anaphoric Identity in Dialogues</title>
      <author><first>Damrin</first><last>Kim</last></author>
      <author><first>Seongsik</first><last>Park</last></author>
      <author><first>Mirae</first><last>Han</last></author>
      <author><first>Harksoo</first><last>Kim</last></author>
      <pages>28–31</pages>
      <abstract>CODI-CRAC 2022 Shared Task in Dialogues consists of three sub-tasks: Sub-task 1 is the resolution of anaphoric identity, sub-task 2 is the resolution of bridging references, and sub-task 3 is the resolution of discourse deixis/abstract anaphora. Anaphora resolution is the task of detecting mentions from input documents and clustering the mentions of the same entity. The end-to-end model proceeds with the pruning of the candidate mention, and the pruning has the possibility of removing the correct mention. Also, the end-to-end anaphora resolution model has high model complexity, which takes a long time to train. Therefore, we proceed with the anaphora resolution as a two-stage pipeline model. In the first mention detection step, the score of the candidate word span is calculated, and the mention is predicted without pruning. In the second anaphora resolution step, the pair of mentions of the anaphora resolution relationship is predicted using the mentions predicted in the mention detection step. We propose a two-stage anaphora resolution pipeline model that reduces model complexity and training time, and maintains similar performance to end-to-end models. As a result of the experiment, the anaphora resolution showed a performance of 68.27% in Light, 48.87% in AMI, 69.06% in Persuasion, and 60.99% on Switchboard. Our final system ranked 3rd on the leaderboard of sub-task 1.</abstract>
      <url hash="9f5e2002">2022.codicrac-1.3</url>
      <bibkey>kim-etal-2022-pipeline</bibkey>
    </paper>
    <paper id="4">
      <title>Neural Anaphora Resolution in Dialogue Revisited</title>
      <author><first>Shengjie</first><last>Li</last></author>
      <author><first>Hideo</first><last>Kobayashi</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>32–47</pages>
      <abstract>We present the systems that we developed for all three tracks of the CODI-CRAC 2022 shared task, namely the anaphora resolution track, the bridging resolution track, and the discourse deixis resolution track. Combining an effective encoding of the input using the SpanBERT<tex-math>_{\text{Large}}</tex-math> encoder with an extensive hyperparameter search process, our systems achieved the highest scores in all phases of all three tracks.</abstract>
      <url hash="10440106">2022.codicrac-1.4</url>
      <bibkey>li-etal-2022-neural-anaphora</bibkey>
    </paper>
  </volume>
</collection>
